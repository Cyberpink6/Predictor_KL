# -*- coding: utf-8 -*-
"""Predictor KL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Npk-e5opIJa8afMccy9pNBF4bP2Gcya-

##CNN para Predicción
"""

import pandas as pd
import numpy as np
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Inicializando datos COMPLETOS
datos = {
    'Hora': [6,7,8,9,10,11,12,13,14,15,16,17,18],
    'Altura': [162.7,171.35,173.49,219.46,257.79,287.01,324.25,339.79,378.78,407.74,434.52,470.56,500.56],
    'Temperatura': [23.05,24.45,25.01,25.5,28.1,29.6,29.63,29.95,29.3,28.4,27.6,27.7,26.9],
    'Presión': [762.05,757.1,756.28,750.31,746.15,740.44,734.09,732.21,729.32,725.18,723.51,720.09,705.36],
    'Humedad': [15.16,15.12,14.01,13.07,15.12,11.21,10.32,10.11,9.28,9.33,8.09,8.11,7.81],
    'Gradiente': [0.142,0.143,0.144,0.116,0.109,0.103,0.091,0.088,0.077,0.0697,0.0635,0.059,0.054],
    'KL_real': [0.247,0.245,0.245,0.204,0.19,0.179,0.162,0.157,0.143,0.134,0.126,0.099,0.112]
}

df = pd.DataFrame(datos)

# USANDO TODAS LAS VARIABLES
X = df[['Hora', 'Altura', 'Temperatura', 'Presión', 'Humedad', 'Gradiente']]
y = df['KL_real']

# Escalar datos - MANTENEMOS LOS NOMBRES
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#  MODELO MEJORADO - MÁS ITERACIONES Y MEJOR CONFIGURACIÓN
modelo_mejorado = MLPRegressor(
    hidden_layer_sizes=(16, 8, 4),  # 3 capas: 16 → 8 → 4 neuronas
    activation='relu',
    solver='adam',
    alpha=0.001,        # Regularización L2
    learning_rate='adaptive',
    learning_rate_init=0.001,  # Tasa de aprendizaje más baja
    max_iter=5000,      # ¡Más iteraciones!
    random_state=42,
    early_stopping=True,
    validation_fraction=0.2,
    n_iter_no_change=100,  # Más paciencia
    tol=1e-6           # Tolerancia más estricta
)

print("Entrenando modelo... Esto puede tomar unos segundos")
# Entrenar modelo mejorado
modelo_mejorado.fit(X_scaled, y)

# Predecir
predicciones_mejoradas = modelo_mejorado.predict(X_scaled)

print("=== MODELO MEJORADO - TODAS LAS VARIABLES ===")
print(f"Variables usadas: Hora, Altura, Temperatura, Presión, Humedad, Gradiente")
print(f"Arquitectura: 6 → 16 → 8 → 4 → 1 neuronas")
print(f"Iteraciones finales: {modelo_mejorado.n_iter_}")
print(f"Pérdida final: {modelo_mejorado.loss_:.6f}")

# Métricas detalladas
mse = mean_squared_error(y, predicciones_mejoradas)
r2 = r2_score(y, predicciones_mejoradas)
error_medio = np.mean(np.abs(y - predicciones_mejoradas))

print(f"\n MÉTRICAS DEL MODELO MEJORADO:")
print(f"Error absoluto medio: {error_medio:.6f}")
print(f"Error cuadrático medio: {mse:.6f}")
print(f"Coeficiente R²: {r2:.6f}")

# Valores de tu ecuación Belette-Ojeda
kl_belette = [0.2461, 0.24, 0.23, 0.204, 0.19, 0.179, 0.162,
              0.157, 0.143, 0.132, 0.125, 0.12, 0.111]

# Comparación detallada
print("\n" + "="*80)
print("COMPARACIÓN DETALLADA: REAL vs MODELO MEJORADO vs BELETTE-OJEDA")
print("="*80)
print("Muestra | Real     | Mejorado  | Belette   | Error Mej | Error Bel")
print("-" * 80)

for i in range(len(y)):
    error_mej = abs(y[i] - predicciones_mejoradas[i])
    error_bel = abs(y[i] - kl_belette[i])
    print(f"{i+1:2d}     | {y[i]:.3f}    | {predicciones_mejoradas[i]:.3f}     | {kl_belette[i]:.3f}     | {error_mej:.4f}      | {error_bel:.4f}")

# Errores comparativos finales
error_belette = np.mean(np.abs(y - kl_belette))
error_redmejorada = np.mean(np.abs(y - predicciones_mejoradas))

mejora_porcentual = ((error_belette - error_redmejorada) / error_belette) * 100

print("\n" + " RESUMEN COMPARATIVO FINAL")
print("="*50)
print(f"Ecuación Belette-Ojeda:    {error_belette:.6f}")
print(f"Red Neuronal Mejorada:     {error_redmejorada:.6f}")
print(f"Mejora:                    {mejora_porcentual:.1f}%")

#  PREDICCIÓN DE EJEMPLO CORREGIDA
print("\n PREDICCIÓN DE EJEMPLO CORREGIDA:")
print("Datos de entrada: Hora=10, Altura=250, Temp=28, Presión=745, Humedad=14, Gradiente=0.11")

# Crear DataFrame con los nombres correctos de las columnas
ejemplo_df = pd.DataFrame({
    'Hora': [10],
    'Altura': [250],
    'Temperatura': [28],
    'Presión': [745],
    'Humedad': [14],
    'Gradiente': [0.11]
})

# Escalar usando el mismo scaler
ejemplo_escalado = scaler.transform(ejemplo_df)
pred_ejemplo = modelo_mejorado.predict(ejemplo_escalado)
print(f"KL predicho: {pred_ejemplo[0]:.4f}")

#  Análisis de importancia de variables
print("\n ANÁLISIS DE CORRELACIONES CON KL:")
correlaciones = df.corr()['KL_real'].sort_values(ascending=False)
for var, corr in correlaciones.items():
    if var != 'KL_real':
        print(f"  {var:12}: {corr:.4f}")

#  VISUALIZACIÓN DE RESULTADOS
print("\n DISTRIBUCIÓN DE ERRORES:")
errores = np.abs(y - predicciones_mejoradas)
print(f"  Error mínimo: {np.min(errores):.6f}")
print(f"  Error máximo: {np.max(errores):.6f}")
print(f"  Desviación estándar de errores: {np.std(errores):.6f}")

#  VERSIÓN MÁS ROBUSTA SI PERSISTEN LOS PROBLEMAS
if modelo_mejorado.n_iter_ >= 5000:
    print("\n  El modelo necesitó muchas iteraciones. Probemos una configuración más robusta...")

    modelo_robusto = MLPRegressor(
        hidden_layer_sizes=(32, 16, 8),  # Más neuronas
        activation='tanh',  # Cambiamos a tanh que suele converger mejor
        solver='adam',
        alpha=0.01,         # Más regularización
        learning_rate='constant',
        learning_rate_init=0.0005,  # Tasa más baja
        max_iter=10000,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.2,
        n_iter_no_change=200,
        tol=1e-7
    )

    print("Entrenando modelo robusto...")
    modelo_robusto.fit(X_scaled, y)
    pred_robusto = modelo_robusto.predict(X_scaled)
    error_robusto = np.mean(np.abs(y - pred_robusto))

    print(f" Modelo robusto - Iteraciones: {modelo_robusto.n_iter_}")
    print(f" Error absoluto robusto: {error_robusto:.6f}")

    # Predicción con modelo robusto
    pred_ejemplo_robusto = modelo_robusto.predict(ejemplo_escalado)
    print(f" KL predicho (modelo robusto): {pred_ejemplo_robusto[0]:.4f}")

"""##Guardar el modelo"""

# GUARDAR MODELO ENTRENADO Y COMPONENTES
import joblib
import pandas as pd
import numpy as np
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler

# 1. ENTRENAR Y GUARDAR EL MODELO (si no lo has hecho ya)
def entrenar_y_guardar_modelo():
    # Tus datos
    datos = {
        'Hora': [6,7,8,9,10,11,12,13,14,15,16,17,18],
        'Altura': [162.7,171.35,173.49,219.46,257.79,287.01,324.25,339.79,378.78,407.74,434.52,470.56,500.56],
        'Temperatura': [23.05,24.45,25.01,25.5,28.1,29.6,29.63,29.95,29.3,28.4,27.6,27.7,26.9],
        'Presión': [762.05,757.1,756.28,750.31,746.15,740.44,734.09,732.21,729.32,725.18,723.51,720.09,705.36],
        'Humedad': [15.16,15.12,14.01,13.07,15.12,11.21,10.32,10.11,9.28,9.33,8.09,8.11,7.81],
        'Gradiente': [0.142,0.143,0.144,0.116,0.109,0.103,0.091,0.088,0.077,0.0697,0.0635,0.059,0.054],
        'KL_real': [0.247,0.245,0.245,0.204,0.19,0.179,0.162,0.157,0.143,0.134,0.126,0.099,0.112]
    }

    df = pd.DataFrame(datos)
    X = df[['Hora', 'Altura', 'Temperatura', 'Presión', 'Humedad', 'Gradiente']]
    y = df['KL_real']

    # Entrenar scaler y modelo
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    modelo = MLPRegressor(
        hidden_layer_sizes=(16, 8, 4),
        activation='relu',
        solver='adam',
        alpha=0.001,
        learning_rate='adaptive',
        max_iter=5000,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.2
    )

    modelo.fit(X_scaled, y)

    # Calcular métricas para el margen de error
    predicciones = modelo.predict(X_scaled)
    error_absoluto_medio = np.mean(np.abs(y - predicciones))
    error_maximo = np.max(np.abs(y - predicciones))

    # 2. GUARDAR TODO EN ARCHIVOS
    joblib.dump(modelo, 'modelo_refraccion.pkl')
    joblib.dump(scaler, 'scaler_refraccion.pkl')
    joblib.dump(error_absoluto_medio, 'error_medio.pkl')
    joblib.dump(error_maximo, 'error_maximo.pkl')

    # Guardar también los nombres de las columnas para referencia
    columnas = list(X.columns)
    joblib.dump(columnas, 'columnas_modelo.pkl')

    print(" Modelo y componentes guardados exitosamente!")
    print(f" Archivos creados:")
    print(f"   - modelo_refraccion.pkl")
    print(f"   - scaler_refraccion.pkl")
    print(f"   - error_medio.pkl")
    print(f"   - error_maximo.pkl")
    print(f"   - columnas_modelo.pkl")
    print(f"\n Métricas del modelo guardado:")
    print(f"   - Error absoluto medio: {error_absoluto_medio:.6f}")
    print(f"   - Error máximo: {error_maximo:.6f}")

# 3. FUNCIÓN PARA CARGAR Y USAR EL MODELO (para la interfaz)
def cargar_modelo():
    """Carga todos los componentes del modelo entrenado"""
    try:
        modelo = joblib.load('modelo_refraccion.pkl')
        scaler = joblib.load('scaler_refraccion.pkl')
        error_medio = joblib.load('error_medio.pkl')
        error_maximo = joblib.load('error_maximo.pkl')
        columnas = joblib.load('columnas_modelo.pkl')

        return modelo, scaler, error_medio, error_maximo, columnas
    except FileNotFoundError:
        print(" No se encontraron los archivos del modelo. Ejecuta primero entrenar_y_guardar_modelo()")
        return None

# 4. FUNCIÓN DE PREDICCIÓN (para la interfaz)
def predecir_kl(hora, altura, temperatura, presion, humedad, gradiente):
    """
    Función para hacer predicciones con el modelo entrenado

    Args:
        hora, altura, temperatura, presion, humedad, gradiente: valores de entrada

    Returns:
        tuple: (prediccion, margen_error_inferior, margen_error_superior)
    """
    componentes = cargar_modelo()
    if componentes is None:
        return None

    modelo, scaler, error_medio, error_maximo, columnas = componentes

    # Crear array de entrada
    entrada = np.array([[hora, altura, temperatura, presion, humedad, gradiente]])
    entrada_df = pd.DataFrame(entrada, columns=columnas)

    # Escalar y predecir
    entrada_escalada = scaler.transform(entrada_df)
    prediccion = modelo.predict(entrada_escalada)[0]

    # Calcular márgenes de error (usando el error absoluto medio)
    margen_inferior = max(0, prediccion - error_medio)  # No menor que 0
    margen_superior = prediccion + error_medio

    return prediccion, margen_inferior, margen_superior, error_medio

# 5. EJECUTAR PARA GUARDAR EL MODELO (solo una vez)
if __name__ == "__main__":
    entrenar_y_guardar_modelo()

    # Probar la función de predicción
    print("\n TEST DE PREDICCIÓN:")
    resultado = predecir_kl(10, 250, 28, 745, 14, 0.11)
    if resultado:
        pred, inf, sup, error = resultado
        print(f"Predicción: {pred:.4f}")
        print(f"Margen de error: ±{error:.4f}")
        print(f"Rango probable: {inf:.4f} - {sup:.4f}")